===CELL 0 code ===
import os
import torch

print(" Checking GPU...")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
else:
    print(" WARNING: No GPU detected")

print("\n Installing dependencies...")
!pip install -q ultralytics==8.0.196
!pip install -q roboflow
!pip install -q opencv-python
!pip install -q shapely
!pip install -q pandas
!pip install -q openpyxl
!pip install -q matplotlib

print(" Installation complete!")

===CELL 1 code ===
from ultralytics import YOLO
from roboflow import Roboflow
import cv2
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from datetime import datetime
import yaml
import shutil
import os

print("libraries imported successfully!")

===CELL 2 code ===
print(" Downloading datasets from Roboflow...")

os.makedirs("/content/datasets", exist_ok=True)

print(" Dataset 1: Roboflow 100 - Solar Panels")
rf1 = Roboflow(api_key="nwjLvwd73Cvdh2afvIVD")
project1 = rf1.workspace("roboflow-100").project("solar-panels-taxvb")
dataset1 = project1.version(2).download("yolov8", location="/content/datasets/solar_panels_rf100")

print("\n Dataset 2: Solar PV Panel Detection")
rf2 = Roboflow(api_key="nwjLvwd73Cvdh2afvIVD")
project2 = rf2.workspace("whereareyousolarpanel").project("solar-pv-panel-detection")
dataset2 = project2.version(5).download("yolov8", location="/content/datasets/solar_pv_detection")

print("\n Dataset 3: WW Solar Panel Dataset")
rf3 = Roboflow(api_key="nwjLvwd73Cvdh2afvIVD")
project3 = rf3.workspace("solar-panel-2d0l1").project("ww-solar-panel")
dataset3 = project3.version(16).download("yolov8", location="/content/datasets/ww_solar_panel")

print("\n All datasets downloaded successfully!")

===CELL 3 code ===
print("Merging datasets...")

combined_dir = Path("/content/datasets/combined_solar")
(combined_dir / "train" / "images").mkdir(parents=True, exist_ok=True)
(combined_dir / "train" / "labels").mkdir(parents=True, exist_ok=True)
(combined_dir / "valid" / "images").mkdir(parents=True, exist_ok=True)
(combined_dir / "valid" / "labels").mkdir(parents=True, exist_ok=True)

def copy_dataset(source_dir, prefix):
    """Copy dataset files with prefix to avoid naming conflicts"""
    source = Path(source_dir)

    train_img_src = source / "train" / "images"
    train_lbl_src = source / "train" / "labels"

    if train_img_src.exists():
        for img_file in train_img_src.glob("*"):
            shutil.copy(img_file, combined_dir / "train" / "images" / f"{prefix}_{img_file.name}")
        print(f"  Copied {len(list(train_img_src.glob('*')))} training images from {prefix}")

    if train_lbl_src.exists():
        for lbl_file in train_lbl_src.glob("*"):
            shutil.copy(lbl_file, combined_dir / "train" / "labels" / f"{prefix}_{lbl_file.name}")

    valid_img_src = source / "valid" / "images"
    valid_lbl_src = source / "valid" / "labels"

    if valid_img_src.exists():
        for img_file in valid_img_src.glob("*"):
            shutil.copy(img_file, combined_dir / "valid" / "images" / f"{prefix}_{img_file.name}")
        print(f"  Copied {len(list(valid_img_src.glob('*')))} validation images from {prefix}")

    if valid_lbl_src.exists():
        for lbl_file in valid_lbl_src.glob("*"):
            shutil.copy(lbl_file, combined_dir / "valid" / "labels" / f"{prefix}_{lbl_file.name}")

copy_dataset("/content/datasets/solar_panels_rf100", "rf100")
copy_dataset("/content/datasets/solar_pv_detection", "pv")
copy_dataset("/content/datasets/ww_solar_panel", "ww")

# Create data.yaml
data_yaml = {
    'path': str(combined_dir),
    'train': 'train/images',
    'val': 'valid/images',
    'names': {
        0: 'solar_panel'
    },
    'nc': 1
}

yaml_path = combined_dir / "data.yaml"
with open(yaml_path, 'w') as f:
    yaml.dump(data_yaml, f, default_flow_style=False)

print(f"\n Combined dataset created at {combined_dir}")
print(f" Total training images: {len(list((combined_dir / 'train' / 'images').glob('*')))}")
print(f" Total validation images: {len(list((combined_dir / 'valid' / 'images').glob('*')))}")

===CELL 4 code ===
import os
from pathlib import Path

print("=" * 80)
print("FIXING CORRUPTED LABELS")
print("=" * 80)

dataset_root = Path("/content/datasets/combined_solar")

def fix_labels_in_split(split_name):
    """Fix all labels in a split by converting any class ID to 0"""
    labels_dir = dataset_root / split_name / "labels"

    if not labels_dir.exists():
        print(f"\n‚ö†Ô∏è {split_name} labels directory not found")
        return

    label_files = list(labels_dir.glob("*.txt"))
    print(f"\nüìÅ Processing {split_name} split: {len(label_files)} label files")

    fixed_count = 0
    corrupted_files = []

    for label_file in label_files:
        try:
            # Read all lines
            with open(label_file, 'r') as f:
                lines = f.readlines()

            if not lines:
                continue

            # Check if any line has class != 0
            needs_fix = False
            fixed_lines = []

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                parts = line.split()
                if len(parts) >= 5:  # Valid YOLO format: class x y w h
                    class_id = int(parts[0])

                    if class_id != 0:
                        needs_fix = True
                        corrupted_files.append(label_file.name)
                        # Replace class with 0
                        parts[0] = '0'

                    fixed_lines.append(' '.join(parts))

            # Write back if needed
            if needs_fix:
                with open(label_file, 'w') as f:
                    f.write('\n'.join(fixed_lines) + '\n')
                fixed_count += 1

        except Exception as e:
            print(f"  ‚ö†Ô∏è Error processing {label_file.name}: {e}")

    print(f"  ‚úì Fixed {fixed_count} corrupted label files")

    if corrupted_files and len(corrupted_files) <= 10:
        print(f"  üìÑ Examples: {', '.join(corrupted_files[:5])}")

    return fixed_count

# Fix all splits
total_fixed = 0
for split in ['train', 'valid', 'test']:
    count = fix_labels_in_split(split)
    if count:
        total_fixed += count

print("\n" + "=" * 80)
print(f"‚úÖ COMPLETE: Fixed {total_fixed} label files total")
print("=" * 80)

# Verify the fix
print("\nüîç Verifying fix...")
for split in ['train', 'valid', 'test']:
    labels_dir = dataset_root / split / "labels"
    if not labels_dir.exists():
        continue

    corrupt_count = 0
    for label_file in labels_dir.glob("*.txt"):
        try:
            with open(label_file, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        if class_id != 0:
                            corrupt_count += 1
                            break
        except:
            pass

    total = len(list(labels_dir.glob("*.txt")))
    if corrupt_count == 0:
        print(f"  ‚úì {split:6s}: All {total} labels valid (class 0 only)")
    else:
        print(f"  ‚úó {split:6s}: Still {corrupt_count}/{total} corrupted labels")

print("\nüí° Next steps:")
print("1. Delete the cache file: rm /content/datasets/combined_solar/valid/labels.cache")
print("2. Re-run your evaluation script")

===CELL 5 code ===
import os
import torch
from ultralytics import YOLO

print("\n" + "=" * 80)
print("EVALUATION")
print("=" * 80)

# Define paths
dataset_root = "/content/datasets/combined_solar"
data_yaml = os.path.join(dataset_root, "data.yaml")
best_weights = "/content/best.pt"

# Verify paths exist
print("\nüîç Verifying paths...")
print(f"Dataset root: {os.path.abspath(dataset_root)}")
print(f"  Exists: {os.path.exists(dataset_root)}")
print(f"Model weights: {os.path.abspath(best_weights)}")
print(f"  Exists: {os.path.exists(best_weights)}")

if os.path.exists(dataset_root):
    print(f"\nüìÅ Dataset structure:")
    for split in ['train', 'valid', 'test']:
        split_path = os.path.join(dataset_root, split)
        if os.path.exists(split_path):
            images_path = os.path.join(split_path, "images")
            labels_path = os.path.join(split_path, "labels")

            img_count = len([f for f in os.listdir(images_path) if f.endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(images_path) else 0
            lbl_count = len([f for f in os.listdir(labels_path) if f.endswith('.txt')]) if os.path.exists(labels_path) else 0

            print(f"  ‚úì {split:6s}: {img_count:4d} images, {lbl_count:4d} labels")

print("\n" + "-" * 80)

# FIX: Allow unsafe loading for YOLO models
print(f"\nüì¶ Loading model...")
try:
    # Method 1: Set torch.serialization to allow ultralytics classes
    torch.serialization.add_safe_globals(['ultralytics.nn.tasks.DetectionModel'])
    model = YOLO(best_weights)
except Exception as e:
    print(f"Method 1 failed, trying Method 2...")
    # Method 2: Force weights_only=False (if you trust the source)
    import torch.nn as nn
    torch.serialization.add_safe_globals([nn.Module])

    # Monkey patch YOLO to use weights_only=False
    original_load = torch.load
    def patched_load(*args, **kwargs):
        kwargs['weights_only'] = False
        return original_load(*args, **kwargs)

    torch.load = patched_load
    model = YOLO(best_weights)
    torch.load = original_load  # Restore original

print("‚úì Model loaded successfully")

# Run validation (using 'val' split since there's no 'test' split)
print(f"\nüîÑ Running validation on validation split...")
try:
    metrics = model.val(data=data_yaml, split='val')

    # Extract metrics
    map50 = float(metrics.box.map50)
    precision = float(metrics.box.mp)
    recall = float(metrics.box.mr)
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)

    print("\n" + "=" * 80)
    print("üìä RESULTS")
    print("=" * 80)
    print(f"  F1 Score:  {f1_score:.4f} {'‚úì' if f1_score >= 0.85 else '‚úó'}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  mAP@50:    {map50:.4f}")
    print("=" * 80)

    # Additional detailed metrics
    print(f"\nüìà Detailed Metrics:")
    print(f"  mAP@50-95: {float(metrics.box.map):.4f}")
    print(f"  Classes:   {metrics.box.nc}")

except Exception as e:
    print(f"\n‚ùå Error during validation: {str(e)}")
    print("\nPlease check:")
    print("1. data.yaml has correct paths")
    print("2. Test split exists in your dataset")
    print("3. Model weights file exists")

===CELL 6 code ===
!pip install -U ultralytics
from ultralytics import YOLO
from datetime import datetime

yaml_path="datasets/combined_solar/data.yaml"
print("Starting YOLOv8 training...\n")

model = YOLO('yolov8m.pt')

training_args = {
    'data': str(yaml_path),
    'epochs': 100,
    'batch': 8,
    'imgsz': 640,
    'patience': 20,
    'save': True,
    'save_period': 10,
    'cache': False,
    'device': 0,
    'workers': 8,
    'project': '/content/runs/detect',
    'name': 'solar_panel_detection',
    'exist_ok': True,
    'pretrained': True,
    'optimizer': 'AdamW',
    'verbose': True,
    'seed': 42,
    'deterministic': True,
    'single_cls': True,
    'rect': False,
    'cos_lr': True,
    'close_mosaic': 10,
    'resume': False,
    'amp': True,
    'fraction': 1.0,
    'profile': False,
    'freeze': None,
    'lr0': 0.001,
    'lrf': 0.01,
    'momentum': 0.937,
    'weight_decay': 0.0005,
    'warmup_epochs': 3.0,
    'warmup_momentum': 0.8,
    'warmup_bias_lr': 0.1,
    'box': 7.5,
    'cls': 0.5,
    'dfl': 1.5,
    'pose': 12.0,
    'kobj': 1.0,
    'label_smoothing': 0.0,
    'nbs': 64,
    'hsv_h': 0.015,
    'hsv_s': 0.7,
    'hsv_v': 0.4,
    'degrees': 0.0,
    'translate': 0.1,
    'scale': 0.5,
    'shear': 0.0,
    'perspective': 0.0,
    'flipud': 0.0,
    'fliplr': 0.5,
    'mosaic': 1.0,
    'mixup': 0.1,
    'copy_paste': 0.0
}

print("\n Training started at:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
print(" Please wait... This will take a while.\n")

results = model.train(**training_args)

print("\n Training completed at:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

===CELL 7 code ===
print("\n Validating trained model...\n")

metrics = model.val()

# Print metrics
print("\n" + "=" * 70)
print("VALIDATION METRICS")
print("=" * 70)
print(f"mAP@0.5: {metrics.box.map50:.4f}")
print(f"mAP@0.5:0.95: {metrics.box.map:.4f}")
print(f"Precision: {metrics.box.mp:.4f}")
print(f"Recall: {metrics.box.mr:.4f}")
print(f"F1 Score: {2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr):.4f}")
print("=" * 70)

# Check if we meet target metrics
f1_score = 2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr)
if f1_score >= 0.85 and metrics.box.map50 >= 0.80:
    print("\n MODEL MEETS TARGET METRICS!")
    print("   F1 Score ‚â• 0.85 ‚úì")
    print("   mAP@0.5 ‚â• 0.80 ‚úì")
else:
    print("\n Model performance below targets. Consider:")
    print("   - Training for more epochs")
    print("   - Adjusting learning rate")
    print("   - Adding more data augmentation")


===CELL 8 code ===
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt

print("\n Saving training logs...\n")

# Load training results
results_csv_path = Path("/content/runs/detect/solar_panel_detection/results.csv")

if results_csv_path.exists():
    results_df = pd.read_csv(results_csv_path)

    # Strip whitespace from column names (YOLO CSVs often have spaces like ' train/box_loss')
    results_df.columns = results_df.columns.str.strip()

    # Save formatted version
    output_csv = "/content/model_training_logs.csv"
    results_df.to_csv(output_csv, index=False)
    print(f" Training logs saved to: {output_csv}")

    # --- Display Summary (Updated columns) ---
    # We replaced 'train/seg_loss' with 'train/cls_loss'
    cols_to_show = ['epoch', 'train/box_loss', 'train/cls_loss',
                    'metrics/precision(B)', 'metrics/recall(B)',
                    'metrics/mAP50(B)']

    # Check if columns exist before printing to avoid errors
    available_cols = [c for c in cols_to_show if c in results_df.columns]

    print("\nüìà Training Summary:")
    print(results_df[available_cols].tail(10))

    # --- Plot Training Curves ---
    def plot_training_curves():
        """Plot training metrics over epochs"""

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Plot 1: Box Loss (Accuracy of the box coordinates)
        if 'train/box_loss' in results_df.columns:
            axes[0, 0].plot(results_df['epoch'], results_df['train/box_loss'], label='Train Box Loss')
        if 'val/box_loss' in results_df.columns:
            axes[0, 0].plot(results_df['epoch'], results_df['val/box_loss'], label='Val Box Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Box Loss (Location Error)')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # Plot 2: Classification Loss (Replacing Segmentation Loss)
        if 'train/cls_loss' in results_df.columns:
            axes[0, 1].plot(results_df['epoch'], results_df['train/cls_loss'], label='Train Cls Loss')
        if 'val/cls_loss' in results_df.columns:
            axes[0, 1].plot(results_df['epoch'], results_df['val/cls_loss'], label='Val Cls Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].set_title('Classification Loss (ID Error)')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        # Plot 3: Precision & Recall
        if 'metrics/precision(B)' in results_df.columns:
            axes[1, 0].plot(results_df['epoch'], results_df['metrics/precision(B)'], label='Precision')
        if 'metrics/recall(B)' in results_df.columns:
            axes[1, 0].plot(results_df['epoch'], results_df['metrics/recall(B)'], label='Recall')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].set_title('Precision & Recall')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

        # Plot 4: mAP (Mean Average Precision)
        if 'metrics/mAP50(B)' in results_df.columns:
            axes[1, 1].plot(results_df['epoch'], results_df['metrics/mAP50(B)'], label='mAP@0.5')
        if 'metrics/mAP50-95(B)' in results_df.columns:
            axes[1, 1].plot(results_df['epoch'], results_df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('mAP')
        axes[1, 1].set_title('Mean Average Precision')
        axes[1, 1].legend()
        axes[1, 1].grid(True)

        plt.tight_layout()
        plt.savefig('/content/training_curves.png', dpi=150, bbox_inches='tight')
        plt.show()
        print(" Training curves saved!")

    plot_training_curves()

else:
    print(" Results CSV not found. Training may have failed.")

===CELL 9 code ===
from ultralytics import YOLO
import matplotlib.pyplot as plt
import cv2
from pathlib import Path

combined_dir = Path("/content/datasets/combined_solar")

print("\n Testing inference on sample images...\n")

best_model = YOLO('/content/runs/detect/solar_panel_detection/weights/best.pt')

val_images = list((combined_dir / "valid" / "images").glob("*"))[:4]

fig, axes = plt.subplots(2, 2, figsize=(15, 15))
axes = axes.ravel()

for idx, img_path in enumerate(val_images):
    results = best_model.predict(str(img_path), conf=0.25, iou=0.45)

    annotated_img = results[0].plot()z
    annotated_img = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)

    axes[idx].imshow(annotated_img)
    axes[idx].set_title(f"Prediction {idx+1}: {len(results[0].boxes)} panels detected")
    axes[idx].axis('off')

plt.tight_layout()
plt.savefig('/content/inference_samples.png', dpi=100, bbox_inches='tight')
plt.show()
print(" Inference test complete!")

